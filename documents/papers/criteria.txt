Goal of the paper:
	Find promising parallelization techniques and list their pro's and cons.
	Determine the challenges and explain which one is the hardest to solve and
	which of the discussed approaches handles it the best.

Criteria suggestions:

Audience:
	researchers - Programmers who dedicate their research to parallelization
		techniques
	professionals - Programmers with experience in writing performant parallel
		applications
	application programmers - People who work in the programming sector
	consumer programmers - Average people who learn programming or scripting for their enjoyment and automating basic tasks

Approach:
	auto - Try to parallelize sequential programs automatically (small gains,
		little effort) auto parallelization is a difficult problem
	programming - Achieve parallelism by re-programming sequential applications
		(large gains, big effort) thinking in parallel terms is difficult for
		people used to sequential programming

Form:
	extension to existing language - macros/functions/abstract data types
	a new language - A design pattern is a recurring problem and a reusable
		solution to it. Separation of concerns, split solution computation and
		orchestration

Host programming language:
	C/C++/C#/Java or domain specific

Model:
	shared memory - memory is shared between workers, locking is required.
		Examples: OpenMP, Pthreads
	message passing - data is shared over a channel between workers, allows
		multi node systems. Example: MPI
	hybrid - combines the strengths of shared memory and message passing for
		applications that can benefit from a two level parallelism hierarchy.
		Introduces new weaknesses. Example: MPI + OpenMP

Extensibility (if applicable):
	Allows defining your own constructs

Flexibility (if applicable):
	Maximum flexibility if provided tools allow you to at least replicate the
	system (with maintainable code)

Application:
	multi-core microprocessors
	multi-core microprocessor network
	graphical processing unit network
	digital signal processing unit network
	fpga network
	hybrid network

Common parallelization techniques:
	task farm
	divide and conquer

http://www.google.com/trends/explore?q=OpenMP%2C+OpenCL%2C+MPI%2C+CUDA%2C+Cilk#cat=0-5&q=OpenMP%2C%20OpenCL%2C%20MPI%2C%20CUDA%2C%20Pthreads&cmpt=q
http://www.google.com/trends/explore?q=parallel%20computing%2C%20distributed%20computing%2C%20parallel%20programming%2C%20%20distributed%20programming&cmpt=q#cat=0-5&q=parallel+computing,+distributed+computing,+parallel+programming,+distributed+programming&cmpt=q
